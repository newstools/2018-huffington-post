Facebook likes democracy. The feeling isn’t mutual. That’s according to a refreshingly honest assessment by Facebook, published Monday as a blog under the site’s recurring “hard questions” feature. In it, Facebook’s global politics and government outreach director Katie Harbath attempts to answer how social media affects democracy. Harbath concedes up front that the initial promise of social media has failed to live up to expectations. “From the Arab Spring to robust elections around the globe, social media seemed like a positive,” she writes. “The last U.S. presidential campaign changed that, with foreign interference that Facebook should have been quicker to identify to the rise of ‘fake news’ and echo chambers.” That sentiment is shared by Cass R. Sunstein, a professor at Harvard Law School, who breaks down Facebook’s particular ailments in a companion essay. In short, Sunstein argues Facebook ― and other social media ― harm democracy by forcing increased “fragmentation, polarization and extremism.” The echo chamber of personalized newsfeeds, a feature Facebook once proudly viewed as a selling point, is instead “a nightmare” for democracy, according to Sunstein. “If you live in an information cocoon, you will believe many things that are false, and you will fail to learn countless things that are true,” Sunstein observes. “That’s awful for democracy. And as we have seen, those with specific interests — including politicians and nations, such as Russia, seeking to disrupt democratic processes — can use social media to promote those interests.” Instead of surrounding ourselves with perspectives, experiences and people that reinforce our worldview, as Facebook as done, a healthy democracy requires the exact opposite. Forgive the block of text, but Sunstein’s three-point argument to that effect is compelling and worth reading in full: First, citizens should be exposed to materials that they would not have chosen in advance. Serendipity is a good thing. Unplanned, unanticipated encounters are central to democracy itself. Such encounters often involve topics and points of view that people have not sought out and perhaps find quite irritating – but that might change their lives in fundamental ways. They are important partly to ensure against fragmentation, polarization, and extremism, which are predictable outcomes of any situation in which like-minded people speak only with themselves. Second, many or most citizens should have a wide range of common experiences. Without shared experiences, a heterogeneous society will have a much more difficult time in addressing social problems. People might see each other as strangers, foreigners, possibly even enemies. Common experiences, emphatically including the common experiences made possible by social media, provide a form of social glue. Societies need such things. Third, citizens should be in a position to distinguish between truth and falsehood – and to know when democratic processes are being manipulated. In democracies, of course, it is fair for people to disagree about what the truth is. But if people are knowingly spreading lies, and if nations are attempting to disrupt other nations, some process should be in place to enable citizens to have access to the truth. These conclusions aren’t exactly revelatory, nor are they new. But Facebook’s acknowledgement that it’s maybe, possibly exacerbating these problems certainly is. Remember that just over a year ago CEO Mark Zuckerberg dismissed as “pretty crazy” the possibility that misinformation on the platform could have influenced the election’s outcome. (He’s since said he regrets the statement.) Before Monday’s blog, the most honest assessments came from an increasingly vocal group of former Facebook executives, who excoriated the platform in various public interviews. Earlier this month, Roger McNamee, an early investor in Facebook and a former advisor to Zuckerberg, told NBC he alerted both Zuckerberg and COO Sheryl Sandberg when he noticed people were being manipulated on the platform during the first Democratic primary in 2016. In their response to him, he said “they treated it like a public relations problem, rather than a substantive issue for the business.” “Making you angry, making you afraid, is really good for Facebook’s business,” he added. “It is not good for America. It’s not good for the users of Facebook.” In December, Facebook’s former vice president of user growth, Chamath Palihapitiya, told an audience at Stanford’s Graduate School of Business he felt “tremendous guilt” about his contributions to the company. “I think we have created tools that are ripping apart the social fabric of how society works,” he said. “The short-term, dopamine-driven feedback loops we’ve created are destroying how society works. ... No civil discourse, no cooperation; misinformation, mistruth. And it’s not an American problem ― this is not about Russians ads. This is a global problem.” That’s similar to the conclusion former Facebook president Sean Parker came to in November, during an interview with Axios’ Mike Allen. “The unintended consequences of a network when it grows to a billion or 2 billion people ― it literally changes your relationship with society, with each other,” Parker said at the time. “It probably interferes with productivity in weird ways,” he continued. “God only knows what it’s doing to our children’s brains.”